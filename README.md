
# Multi-LLM-Evaluation-Framework

A professional and streamlined framework for evaluating the performance of multiple large language models (LLMs) on a single generated question. This tool facilitates fair comparison and ranking of model responses using another LLM as a judge.

---

## ğŸ” Purpose

This project helps benchmark how different LLMs respond to a **challenging and nuanced prompt**. It:

* Automatically generates a single complex question using a host LLM.
  
* Sends that question to various LLMs (contestants).
  
* Collects their responses.
  
* Sends all answers to a judge LLM for ranking.
  
* Outputs results in a clean, ranked format.

---

## ğŸ§  Models Used

* **Question Generator:** Groq (LLaMA 3.3 70B Versatile)
  
* **Contestant LLMs:**

  * LLaMA 3.3 70B Versatile
    
  * Gemini 2.0 Flash
    
  * DeepSeek R1 Distill LLaMA 70B
    
  * Qwen QWQ 32B
    
  * LLaMA 3 8B
    
  * Mistral SABA 24B
    
* **Judge:** GPT-4o (via OpenAI)

---

## âš™ï¸ How It Works

1. A **challenging question** is created by LLaMA 3.3 70B.
   
2. The question is sent to each LLM, and their responses are collected.
   
3. The responses are **combined** and sent to GPT-4o for evaluation.
   
5. GPT-4o returns a **JSON ranking** of the competitors.

---
# Multi-LLM Evaluation Workflow:
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚    Host LLM (70B)   â”‚
                 â”‚ e.g., LLaMA3-70B    â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Generates a complex, nuanced       â”‚
        â”‚ question to challenge other LLMs   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Question sent to Competitors  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                â–¼                            â–¼            â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLaMA3-8B   â”‚  â”‚ Qwen-32B       â”‚          â”‚ Gemini-2 Flash â”‚  â”‚ Mistral  â”‚ â”‚ Mixtral  â”‚
â”‚ (Groq API)  â”‚  â”‚ (Groq API)     â”‚          â”‚ (Google API)   â”‚  â”‚ (Groq)   â”‚ â”‚ (Groq)   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚               â”‚                            â”‚                   â”‚            â”‚
     â–¼               â–¼                            â–¼                   â–¼            â–¼
Answers captured and stored â Combined into a unified format
                          â”‚
                          â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Answers sent to Judge LLM (GPT)  â”‚
         â”‚         e.g., GPT-4o               â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Judge evaluates responses based on:       â”‚
       â”‚ - Depth                                   â”‚
       â”‚ - Clarity                                 â”‚
       â”‚ - Relevance & Nuance                      â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚   Final Ranking Returned   â”‚
             â”‚   in JSON Format           â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
---

## ğŸ› ï¸ Tech Stack

* Python (Colab Notebook)
  
* Groq SDK
  
* OpenAI SDK
  
* Google Generative AI SDK
  
* Markdown rendering (IPython display)

---

## ğŸ§ª Example Output

```json
{"results": ["2", "5", "1", "4", "6", "3"]}
```

This indicates the ranking of each model by answer quality.

---

## âœ… Ideal Use Cases

* Evaluating and comparing new LLMs
  
* Research experiments for LLM capabilities
  
* Automated QA benchmarking pipelines

---

## ğŸ“Œ Notes

* All responses and rankings are **subjective** to the judge model.
  
* This framework is designed to be **extensible** â€“ more LLMs can be added easily.

---

## ğŸ§‘â€ğŸ’» Author

**Faiaz Ahmed**

---

## ğŸ“œ License

MIT License
